exp_recoded = fct_relevel(exp_recoded, "12", after = 11)) %>%
select(-exp) %>%
ungroup()
```
mean(as.numeric(as.character(age))
mean_age <- as.numeric(as.character(raw_d_munged$age, na.rm = T))
mean_age <- as.numeric(as.character(raw_d_munged$age, na.rm = T))
mean_age
mean_age <- mean(as.numeric(as.character(raw_d_munged$age)), na.rm = T)
mean_age
setwd("~/Documents/research/Projects/conVar/paper_figures/dutch_eng_association_plot")
# plot bar graph of association weights of cheese and ear in dutch and english
library(tidyverse)
library(forcats)
### read in data - these are all from Gary
### ENGLISH ASSOCIATIONS
eng_raw = read_csv("data/correlation-swow/correlation_full_en.csv") %>%
select(-1) %>%
mutate(language = "English")
# this isn't critical
dutch_dict <- read_csv("data/mmc2.csv") %>%
select(Word, Translation) %>%
filter(Translation != "") %>%
mutate(Translation = tolower(Translation))
### DUTCH ASSOCATIONS ASSOCIATIONS
nl_raw = read_csv("data/correlation-swow/correlation_full_nl.csv") %>%
select(-1) %>%
mutate(language = "Dutch") %>%
left_join(dutch_dict, by = c("cue" = "Word")) %>%
rename(cue_eng = Translation) %>%
left_join(dutch_dict, by = c("target" = "Word")) %>%
rename(target_eng = Translation)
# My translated list of top 10 associations
MIN_RANK <- 11
my_dict <- read_csv("data/final_associations.csv")
#### JEALOUSY PLOT
target_word <- "jealousy"
current_dict = my_dict %>%
filter(equiv_index_1 < MIN_RANK,
item == target_word) %>%
select(-equiv_index_1) %>%
distinct()
target_words <- filter(current_dict, item == target_word)
eng_plot <-  get_eng_plot_data(eng_raw, current_dict, target_word, target_words)
nl_plot <- get_nl_plot_data(nl_raw, current_dict, target_word, target_words)
plot_data <- eng_plot %>%
bind_rows(nl_plot) %>%
mutate(target = as.factor(target))  %>%
mutate(target = fct_relevel(target, rev(c("envy", "green/green-eyed", "anger/rage", "hate/hatred", "love", "woman/women/wife",
"emotion", "relationship", "bad", "man", "argument"))),
target = fct_recode(target,
"envy \n (afgunst/nijd)" = "envy",
"green(-eyed) \n (groen)" = "green/green-eyed" ,
"anger/rage \n (woede)" = "anger/rage",
"hate/hatred \n (haat)" = "hate/hatred" ,
"love \n (liefde)" = "love" ,
"woman/wife \n (vrouw(en))" = "woman/women/wife",
"emotion \n (emotie)" = "emotion" ,
"relationship \n (relatie)" = "relationship" ,
"bad \n (slecht)" = "bad",
"man \n (man)" = "man",
"argument \n (ruzie)" = "argument"),
language = fct_rev(language))
pdf("jealousy.pdf", width = 8, height = 4.6)
ggplot(plot_data,
aes(x = target, y = swow_normalized,
fill = target)) +
geom_bar(stat = "identity") +
facet_grid(~language) +
coord_flip() +
theme_minimal() +
theme(legend.position = "none",
axis.text.y = element_text(size = 7)) +
ggtitle("jealousy (jaloezie)") +
ylab("normalized conditional probability") +
xlab("word association")
dev.off()
plot bar graph of association weights of cheese and ear in dutch and english
library(tidyverse)
library(forcats)
### read in data - these are all from Gary
### ENGLISH ASSOCIATIONS
eng_raw = read_csv("data/correlation-swow/correlation_full_en.csv") %>%
select(-1) %>%
mutate(language = "English")
# this isn't critical
dutch_dict <- read_csv("data/mmc2.csv") %>%
select(Word, Translation) %>%
filter(Translation != "") %>%
mutate(Translation = tolower(Translation))
### DUTCH ASSOCATIONS ASSOCIATIONS
nl_raw = read_csv("data/correlation-swow/correlation_full_nl.csv") %>%
select(-1) %>%
mutate(language = "Dutch") %>%
left_join(dutch_dict, by = c("cue" = "Word")) %>%
rename(cue_eng = Translation) %>%
left_join(dutch_dict, by = c("target" = "Word")) %>%
rename(target_eng = Translation)
# My translated list of top 10 associations
MIN_RANK <- 11
my_dict <- read_csv("data/final_associations.csv")
get_eng_plot_data <- function(eng_raw, current_dict, target_word, target_words){
eng_plot <- eng_raw %>%
filter(cue == target_word) %>%
filter(target %in% target_words$english) %>%
left_join(current_dict %>%  # get all dutch translation
select(english, equiv_index_1_name) %>% distinct(),
by = c("target" = "english")) %>%
mutate(equiv_index = ifelse(is.na(equiv_index_1_name),
target, equiv_index_1_name)) %>%
group_by(equiv_index) %>%
summarize(swow = sum(swow)) %>%
mutate(swow_normalized = swow/sum(swow),
language = "English") %>%
rename(target = equiv_index)
return(eng_plot)
}
get_nl_plot_data <- function(nl_raw, current_dict, target_word, target_words){
nl_plot <- nl_raw %>%
filter(cue_eng == target_word) %>%
filter(target %in% target_words$dutch) %>%
left_join(current_dict %>% select(dutch, english, equiv_index_1_name) %>% distinct(),
by = c("target" = "dutch")) %>%
select(target, english, swow, equiv_index_1_name) %>%
mutate(equiv_index = ifelse(is.na(equiv_index_1_name),
english, equiv_index_1_name)) %>%
group_by(equiv_index) %>%
summarize(swow = sum(swow)) %>%
mutate(swow_normalized = swow/sum(swow),
language = "Dutch") %>%
rename(target = equiv_index)
return(nl_plot)
}
#### JEALOUSY PLOT
target_word <- "jealousy"
current_dict = my_dict %>%
filter(equiv_index_1 < MIN_RANK,
item == target_word) %>%
select(-equiv_index_1) %>%
distinct()
target_words <- filter(current_dict, item == target_word)
eng_plot <-  get_eng_plot_data(eng_raw, current_dict, target_word, target_words)
nl_plot <- get_nl_plot_data(nl_raw, current_dict, target_word, target_words)
plot_data <- eng_plot %>%
bind_rows(nl_plot) %>%
mutate(target = as.factor(target))  %>%
mutate(target = fct_relevel(target, rev(c("envy", "green/green-eyed", "anger/rage", "hate/hatred", "love", "woman/women/wife",
"emotion", "relationship", "bad", "man", "argument"))),
target = fct_recode(target,
"envy \n (afgunst/nijd)" = "envy",
"green(-eyed) \n (groen)" = "green/green-eyed" ,
"anger/rage \n (woede)" = "anger/rage",
"hate/hatred \n (haat)" = "hate/hatred" ,
"love \n (liefde)" = "love" ,
"woman/wife \n (vrouw(en))" = "woman/women/wife",
"emotion \n (emotie)" = "emotion" ,
"relationship \n (relatie)" = "relationship" ,
"bad \n (slecht)" = "bad",
"man \n (man)" = "man",
"argument \n (ruzie)" = "argument"),
language = fct_rev(language))
pdf("jealousy.pdf", width = 8, height = 4.6)
ggplot(plot_data,
aes(x = target, y = swow_normalized,
fill = target)) +
geom_bar(stat = "identity") +
facet_grid(~language) +
coord_flip() +
theme_minimal() +
theme(legend.position = "none",
axis.text.y = element_text(size = 7)) +
ggtitle("jealousy (jaloezie)") +
ylab("normalized conditional probability") +
xlab("word association")
dev.off()
#### CHEESE PLOT
target_word <- "cheese"
current_dict = my_dict %>%
filter(equiv_index_1 < MIN_RANK,
item == target_word) %>%
select(-equiv_index_1) %>%
distinct()
target_words <- filter(current_dict, item == target_word)
eng_plot <-  get_eng_plot_data(eng_raw, current_dict, target_word, target_words)
nl_plot <- get_nl_plot_data(nl_raw, current_dict, target_word, target_words)
plot_data <- eng_plot %>%
bind_rows(nl_plot) %>%
mutate(target = as.factor(target)) %>%
mutate(target = fct_relevel(target, rev(c("cheddar", "mouse", "food", "brie",
"yellow", "tasty", "orange", "pizza",
"wine", "Swiss", "bread", "holes",
"cow", "Netherlands", "stench"))),
target = fct_recode(target,
"bread \n (brood)" = "bread",
"brie \n (brie)" = "brie" ,
"cheddar \n (cheddar)" = "cheddar",
"cow \n (koe)" = "cow" ,
"food \n (eten)" = "food" ,
"holes \n (gaten/gaatjes)" = "holes" ,
"mouse \n (muis)" = "mouse",
"Netherlands \n (Holland/Netherlands)" = "Netherlands",
"orange \n (oranje)" = "orange",
"pizza \n (pizza)" = "pizza" ,
"stench \n (stank)" = "stench" ,
"Swiss \n (Zwitsers/Zwitserse)" = "Swiss",
"tasty \n (lekker)" = "tasty",
"wine \n (wijn)" = "wine",
"yellow \n (geel)" = "yellow"),
language = fct_rev(language))
pdf("cheese.pdf", width = 8, height = 4.6)
ggplot(plot_data,
aes(x = target, y = swow_normalized,
fill = target)) +
geom_bar(stat = "identity") +
facet_grid(~language) +
coord_flip() +
theme_minimal() +
theme(legend.position = "none",
axis.text.y = element_text(size = 7)) +
ggtitle("cheese (kaas)") +
ylab("normalized conditional probability") +
xlab("word association")
dev.off()
setwd("~/Documents/research/Projects/XTMEM/paper")
setwd("~/Documents/research/Projects/XTMEM/paper")
ggplot(LF_means_long_recoded,
aes(x = exp_recoded, y = mean, group = condition, fill = condition)) +
geom_bar(position = "dodge", stat = "identity") +
geom_linerange(aes(ymin = ci_lower,
ymax = ci_upper),
position = position_dodge(width = .9)) +
facet_grid(. ~ condition2, scales = "free", drop = TRUE,  space = "free_x") +
scale_fill_discrete(labels = c("one", "three")) +
ylim(0, 1) +
ylab("Prop. basic-level choices") +
xlab("Experiment") +
theme(strip.text = element_text(size = 10),
strip.background = element_rect(fill = "grey"),
legend.position = c(0.92, 0.79),
legend.text = element_text(size = 9),
legend.title = element_text(size = 10),
legend.key.size = unit(.5, "cm"),
legend.background =  element_rect(color = "black", size = 0.5)) +
ggthemes::theme_few() +
ggthemes::scale_color_solarized()
---
title             : "Still suspicious: The suspicious coincidence effect revisited"
shorttitle        : "The suspicious coincidence effect revisited"
author:
- name          : "Molly L. Lewis"
affiliation   : "1,2"
corresponding : yes    # Define only one corresponding author
email         : "mollylewis@uchicago.edu"
- name          : "Michael C. Frank"
affiliation   : "3"
email         : "mcfrank@stanford.edu"
affiliation:
- id            : "1"
institution   : "Computation Institute, University of Chicago"
- id            : "2"
institution   : "Department of Psychology, University of Wisconsin, Madison"
- id            : "3"
institution   : "Department of Psychology, Stanford University"
note: |
$^*$To whom correspondence should be addressed. Email: mollylewis@uchicago.edu
abstract: |
Enter abstract here. Each new line herein must be indented, like this line.
keywords          : "word learning, Bayesian inference, meta-analysis, concepts"
wordcount         : "X"
bibliography      : ["r-references.bib"]
header-includes:
- \usepackage{setspace}
- \usepackage{float}
- \usepackage{graphicx}
- \AtBeginEnvironment{tabular}{\singlespacing}
- \usepackage{pbox}
- \usepackage{hyphsubst}
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---
```{r load_utility_packages, include = FALSE}
library(papaja)
library(rmarkdown)
library(broom)
library(tidyverse)
library(langcog)
library(jsonlite)
library(stringr)
library(forcats)
library(knitr)
library(kableExtra)
```
```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning = FALSE,
message = FALSE,
cache = FALSE,
fig.pos = "t!")
```
## Intro
Suppose you're learning a foreign language and you learn that a chili pepper can be called "wug." What does "wug" mean? This question is challenging  of course because the same object can be refered to by many different labels depending on the level of abstraction that the speaker wishes to convey. You could refer to the same chili pepper using the labels "chili pepper" at the subordinate level, "pepper" at the basic level, or "vegetable" at the superodinate level. For the naive learner, this ambiguity poses a fundamental challenge for inferring the meaning of the word since every instance of "wug" that the learner hears is consistent with extensions at all three levels of abstraction. Furthermore, children rarely receive the kind of negative evidence ("this is *not* a wug") that would help disambiguate the word's meaning. Yet, despite the apparent difficulty of the learning problem, children sucessfully learn words at multiple levels of abstraction [@markman1990constraints].
Xu and Tenenbaum (2007a; henceforth XT) provide one account as to how children might learn such words without relying on negative evidence. Within a Bayesian framwork, they suggest that learners implicitly consider the likelihood of hearing different word-objects pairs under different hypotheses about the extension of a word. One consequence of this assumption is that learners should be sensitive to the number of word-objects pairs they observe when determining a word's meaning. In particular, XT predict that a learner should think that it would be a "suspcious coincidence" to observe three subordinate examples (e.g., chili peppers) with the word "wug" if the true meaning of the word were at the basic level (e.g., pepper). More generally, they predict that a learner should be more likely to generalize narrowly to the subordinate level when they observe more word-object pairs. In two experiments, they find that both adults and children show exactly this pattern.
This finding has been foundational to a number of other more recent findings.
* strong sampling
* Gweon, Tenenbaum, and Schulz (2009) [TO DO]
In a follow-up study to XT, Spencer, Perone, Smith, and Samuelson (2011; henceforth SPSS) offer an alternative explanation for the suspicious coincidence effect. They argue that the effect can be accounted for by basic memory processes in which the co-occurence of objects in time and space highlights differences across exemplars, thus leading to increased conceptual discrimination. They predict that this increased conceptual discrimination should make it more likely for participants to generalize to the subordinate level when more subordinate category exemplars are observed---the suspicious coincidence pattern observed by XT. SPSS test this possibility by replicating the original XT experiments with slightly different design parameters. The theoretically-motivated design difference was the the timing of the training exemplars: Rather than presenting the learning exemplars simulateneously, they present them in sequence such that only one learning exemplar is visible at a time. The sequential presentation of objects, the argue, more closely reflects the experience of learners in the real world who encounter objects in time and space.
In a series of experiments, SPSS replicate XT's finding with simultaneous presentation of the learning exemplars, but fail to replicate  with  sequential presentation. In fact, they observe a reversal of the effect under sequential presentation conditions, such that participants were more likely to generalize to the basic level when more subordinate exemplars were presented.
These findings are surprising in part because it is not clear that effects of basic memory processes should lead to broader generalization. While SPSS argue that simultaneous presentation highlights differences across exemplars, others have suggested that this method highlights their commonalities and increases memory consolidation, thus predicting *greater* generalization in the sequential condition [@lawson2017influence]. Indeed, several findings suggest that preschoolers demonstrate the suspicious coinccendince effect when generalizing properties under sequential presentation [@lawson2014three], and that this effect disappears under simultanous presentation [@lawson2017influence; @lawson2011s]. [check these citations - I think they're out of order]
On the other hand, there is reason to think that estimates of rational inference in word learning may be over-estimated. In other work, we find that a related effect predicted by XT [@xu2007]---strong versus weak sampling---appears to be much smaller in magnitude relative to the original estimate [@lewis2016understanding].
Given the theoretical importance of the suspicious coincidence effect and the conflicting empirical picture, we sought to replicate the suspicious coincidence effect. We report 12 experiments, 10 of which were pre-registered, that varied four design aspects that differed between XT and SPPS: Presentation timing, trial order, blocking, and label consistency. We recover the suspicious coincidence effect with a large effect size in both sequential and simultaneous presentation conditions. The effect only occurs, however, in experiments where the trial with one exemplar is presented *before* the key trial with three subordinate-consistent exemplars (the "suspicious coincidence"). We attribute this difference to participants' awareness of the possibility of subordinate generalizations following the three-exemplar trial; in these conditions, we see a high level of subordinate generalizations even for the one-exemplar trial (leading to the absence of a difference between conditions). In sum, although we replicate SPSS exactly, our full set of studies leads us to a different interpretation of the data: Contra SPSS, we conclude that the "suspicious coincidence" effect is robust to sequential presentation. The effect is sensitive to some features of the general experimental context, however, suggesting a potential interpretation in terms of the pragmatics of the task.
# Methods
We report how we determined our sample size, all manipulations, and all measures in the study. All stimuli, experimental code, sample sizes, and analyses were pre-registered with the exception of Exps. 4 and 8, and all are publically available (https://osf.io/yekhj/).
## Participants
```{r read_in_data}
### all data
all_d <- read_csv("../data/anonymized_data/all_data_munged_A.csv")
### data with repeat participants excluded
all_d_filtered <- read_csv("../data/anonymized_data/no_dups_data_munged_A.csv")
## key to experiment factors
exp_key <- read_csv("../data/experiment_key.csv") %>%
mutate(order = gsub("\"", "", order))# this is because excel is dumb, grrr
```
```{r, get_n_unique_participants}
n_unique <- all_d_filtered %>%
distinct(exp, subids) %>%
summarize(n = n())
n_total <- all_d %>%
distinct(exp, subids) %>%
summarize(n = n())
percent_duplicates <- round((n_total - n_unique)/n_total, 2) * 100
# https://mlewis.shinyapps.io/xtmem_SI/"
```
Fifty participants were recruited on Amazon Mechanical Turk for each of our 12 experiments (N = 600), and paid 40-50 cents for their participation. Across all 12 experiments, `r percent_duplicates[[1]]`% of participants completed more than one experiment. We report data from all participants in the Main Text, but the pattern of reported findings holds when these participants are excluded (see SI)\footnote{Supplemental information can be found at "}.
We determined our sample size on the basis of a pre-registered power calculation using a meta-analytic estimate of the effect size from studies conducted by XT and SPSS. The chosen sample size was approximately twice the estimated sample size necessary to obtain a power of .99.
```{r, out.width = "50%", fig.align = "center", fig.cap = "Sample stimuli. Three superodinate (top), basic (middle), and subordinate (bottom) exemplars from the vegetable category."}
include_graphics("figs/stim.pdf")
```
## Stimuli
Our stimuli closely resembeled that of XT and SPSS. [this pics just came from search the internet] The linguistic stimuli were 12 one-syllable novel labels (e.g., "wug"), and the referent objects were three sets of 15 pictures from different basic level categories (vegetables, vehicles and animals). Within each category, five were subordinate exemplars (e.g., green pepper), four were basic level exemplars (e.g., peppers), and six were superordinate exemplars (e.g., vegetables; Fig.\ 1). The exemplars were divided into a learning and generalization set. For each category, the learning set consisted of 3 subordinate, 2 basic, and 2 superordinate pictures presented in different combinations on different trials (see Procedure). The generalization set for each category consisted of the remaining 8 pictures. The learning and generalization sets were the same for all participants.
## Procedure
```{r get_es}
# remap condition values and select relevant conditions
all_d_clean <- all_d %>%
mutate(condition = as.factor(condition),
exp = as.character(exp),
condition = fct_recode(condition,
three_basic = "3bas",
three_subordinate = "3sub",
three_superordinate = "3sup")) %>%
filter(condition == "one" | condition == "three_subordinate") %>%
select(exp, everything())
# mean across categories (subject means)
ms <- all_d_clean %>%
gather(variable, value, c(prop_sub, prop_bas, prop_sup)) %>%
mutate(value = as.numeric(value)) %>%
filter(variable == "prop_bas") %>%
group_by(condition, exp, subids) %>%
summarize(value = mean(value))
# means across participants (condition means)
LF_means_wide <- ms %>%
spread(condition, value) %>%
group_by(exp) %>%
summarize(m_one = mean(one),
sd_one = sd(one),
m_3sub = mean(three_subordinate),
sd_3sub = sd(three_subordinate),
n = n()) %>%
left_join(exp_key)
LF_effect_sizes <- LF_means_wide %>%
do(data.frame(d = compute.es::mes(.$m_one, .$m_3sub, .$sd_one,
.$sd_3sub, .$n, .$n, verbose = F)$d,
d_var = compute.es::mes(.$m_one, .$sd_3sub, .$sd_one,
.$sd_3sub, .$n, .$n, verbose = F)$var.d)) %>%
mutate(high = d + (1.96*d_var),
low = d - (1.96*d_var),
es_type = "nonpaired",
exp_recoded = LF_means_wide$exp_recoded) %>%
left_join(LF_means_wide %>% select(exp_recoded, n)) %>%
select(exp_recoded, n, everything())
```
```{r}
exp_table <- exp_key %>%
slice(1:12) %>%
left_join(LF_effect_sizes) %>%
mutate(d_string = paste0(d," [", round(low,2), ", ", round(high, 2) , "]"),
direct_replication_of_string = ifelse(is.na(direct_replication_of),
"", direct_replication_of),
timing = str_replace_all(timing, "sequential", "seq."),
timing = str_replace_all(timing, "simultaneous", "simult."),
blocking = str_replace_all(blocking, "random", "pseudo-random"),
one_3sub_label = str_replace_all(one_3sub_label, "different", "diff."),
exp_recoded = as.numeric(exp_recoded)) %>%
#direct_replication_of_string = replace(direct_replication_of_string, exp_recoded == 1,"XT E1/E2[note]" )) %>%
select(exp_recoded, n, timing, order, blocking,
one_3sub_label, d_string, direct_replication_of_string) %>%
arrange(exp_recoded)
kable(exp_table, align = c('c', 'r', 'r', 'r', "r", "r", "r", "r"),
caption = "Summary of our 12 experiments.",
col.names = c("Exp.", "N", "Timing", "Order",
"Blocking", "Label", "Effect Size", "Original \nExp."),
format = "latex", booktabs = TRUE) %>%
kable_styling(font_size = 12) %>%
add_header_above(c(" " = 2, "Manipulations" = 4, " ")) %>%
add_footnote("N = sample size; Timing = presentation timing (sequential or simultaneous); Order = relative ordering of 1 and 3 subordinate trials; Blocking = trials blocked by category or pseudo-random; Label = same or different label in 1 and 3 trials; Effect size = Cohen's d [95% CI]; Original Exp.\ = corresponding experiment from prior literature.", notation = "number")
```
Participants were first introduced to a picture of a character ("Mr. Frog") and instructions describing the task. They were told that the character speaks a different language and their job was to help the character find the toys he wants. Participants then advanced to the main experiment, which consisted of a series of 12 trials on separate screens. On each trial,  one or three learning exemplars from one of the three stimulus categories appeared at the top of the screen, along with the following instructions: "Here [is a wug/are three wugs]. Can you give Mr. Frog all of the other wugs?." Below the learning exemplars, 24 generalization exemplars (8 from each of the 3 categories) were displayed in a 4*x*6 grid. The order of generalizaiton pictures was randomized across trials. Participants were instructed to select the target category members ("To give a wug, click on it below. When you have given all the wugs, click the Next button."). When an exemplar was selected, a red box appeared around the picture, and participants were allowed to change their selections by clicking on the picture a second time. The learning exemplars remained visible at the top of the screen during the generalization task. Once they had made their selections, participants advanced to the next trial by clicking the "Next" button.
There were four trial types distinguished by the number and semantic level of the learning exemplars: one subordinate exemplar, three subordinate exemplars, three basic exemplars, and three superordate exemplars. Each participant completed each trial type for each of the three stimulus categories (vegetables, vehicles, and animals).
Across 12 experiments, we manipulated four aspects of the trial design that differed between XT and SPSS (summarized in Table 1): Presentation timing (simultaneous vs. sequential), trial order (1-3 vs. 3-1), label (same vs. different), and blocking (blocked vs. pseudo-random).\footnote{All experiments can be viewed directly at XXX.}. We describe each of these factors in more detail below
### Presentation Timing
Presentation timing was the key, theoretically motivated experimental design difference between the XT (E1 and E2\footnote{XT E1 and E2 differed in the age of participants (adults vs.\ children), but we collapse across this difference for the present analyses.}) and SPSS (E2 and E3). In XT, the learning exemplars were presented statically and simultaneously, while in SPSS, participants saw a sequence of individual exemplars with each exemplar visible only for 1s at a time. In the sequential design, three-exemplar learning trials displayed pictures at three different locations (left, middle, and right) in a sequence that repeated twice, for a total of 6s.
We reproduced these design aspects in the simultaneous and sequential versions of our experiments. In the single-exemplar, sequential trials, the exemplar appeared (1s) and disappeared (1s) for three repetitions. The generalization pictures did not appear in the sequential condition until after the training pictures has appeared for 6 seconds, but  remained visible as participants selected generalization exemplars.
### Trial order
In XT, the three one-subordinate trials occured first followed by all other trial types. In contrast, in SPSS (E2 and E3), the three-subordinate trials occured first. SPSS's replication of XT's simultanous design (SPSS E1) used the 1-3 ordering.[This isn't actually quite true: "The first block of trials always involved either one exemplar or three subordinate-level exemplars from each domain. The remaining blocks of trials were randomly ordered for each participant"... so 1 exemplar trials were first only half the time?].
### Labels
XT used the same label for each category for the three-subordinate and one-suborinate trials. SPSS used a different novel label on each of the 12 trials, such that the three-subordinate and one-subordinate trials were refered to with  distinct labels. We reproduced these two design choices, and also randomized labels across trials.
### Blocking
Finally, the studies by XT and SPSS differ in terms of whether the trials were blocked by trial type: In XT, the first three trials were a block of one-subordinate trials and the remaining 9 trials were randomized, whereas SPSS blocked all four trial types. We also reproduced these designs, randomizing within each block.
## Data analysis
The key prediction of the suspicious coincidence effect is that participants should generalize to the basic level more often in one-subordinate trials relative to three-subordinate trials. To measure this, for each trial, we calculated the proportion generalizations to subordinate exemplars within the same category (out of 2) and basic exemplars within the same category (out of 2), and averaged across categories for each participant. We estimated the difference between the one-subordinate and three-subordinate conditions by calculating an effect size (Cohen's *d*) for each experiment. We then estimated the influence of each our design manipulations on the overall effect size by fitting a random-effect meta-analytic model with each of our four manipulations as fixed effects. We used the metafor package [@R-metafor] in R to fit our meta-analytic models.
# Results
```{r fig.height = 3, fig.cap = "Mean proportion generalizations to basic level exemplars in the one (pink) and three (green) subordinate exemplar conditions for all 12 of our experiments. Each facet corresponds to a pairing of presentation timing (simultaneous vs. sequential) and trial order (1-3 vs. 3-1). Error bars are bootstrapped 95% confidence intervals."}
# mean across participants with CIS (condition means)
LF_means_long <- ms %>%
group_by(condition, exp) %>%
# multi_boot_standard(column = "value")  %>%
summarise(mean = mean(value),
error = qt(0.975,df=n()-1)*sd(value)/sqrt(n()),
ci_lower = mean - error,# FIXME to bootstrap
ci_upper = mean + error) %>%
left_join(exp_key)
# relabel for fig
LF_means_long_recoded <- LF_means_long %>%
ungroup()  %>%
mutate(exp_recoded = fct_relevel(exp_recoded, as.character(1:12)),
condition2 = paste(timing, order),
condition2 = fct_recode(condition2, "sequential \n1-3 order" = "sequential 1-3",
"simultaneous \n1-3 order" = "simultaneous 1-3",
"sequential \n3-1 order" = "sequential 3-1",
"simultaneous \n3-1 order" = "simultaneous 3-1"),
condition2 = fct_relevel(condition2, c("simultaneous \n1-3 order", "sequential \n1-3 order",
"simultaneous \n3-1 order", "sequential \n3-1 order")),
condition = fct_relevel(condition, c("one", "three")))
# prop basic mea
# prop basic means fig
ggplot(LF_means_long_recoded,
aes(x = exp_recoded, y = mean, group = condition, fill = condition)) +
geom_bar(position = "dodge", stat = "identity") +
geom_linerange(aes(ymin = ci_lower,
ymax = ci_upper),
position = position_dodge(width = .9)) +
facet_grid(. ~ condition2, scales = "free", drop = TRUE,  space = "free_x") +
scale_fill_discrete(labels = c("one", "three")) +
ylim(0, 1) +
ylab("Prop. basic-level choices") +
xlab("Experiment") +
theme(strip.text = element_text(size = 10),
strip.background = element_rect(fill = "grey"),
legend.position = c(0.92, 0.79),
legend.text = element_text(size = 9),
legend.title = element_text(size = 10),
legend.key.size = unit(.5, "cm"),
legend.background =  element_rect(color = "black", size = 0.5))
ggplot(LF_means_long_recoded,
aes(x = exp_recoded, y = mean, group = condition, fill = condition)) +
geom_bar(position = "dodge", stat = "identity") +
geom_linerange(aes(ymin = ci_lower,
ymax = ci_upper),
position = position_dodge(width = .9)) +
facet_grid(. ~ condition2, scales = "free", drop = TRUE,  space = "free_x") +
scale_fill_discrete(labels = c("one", "three")) +
ylim(0, 1) +
ylab("Prop. basic-level choices") +
xlab("Experiment") +
theme(strip.text = element_text(size = 10),
strip.background = element_rect(fill = "grey"),
legend.position = c(0.92, 0.79),
legend.text = element_text(size = 9),
legend.title = element_text(size = 10),
legend.key.size = unit(.5, "cm"),
legend.background =  element_rect(color = "black", size = 0.5)) +
ggthemes::theme_few()
ggplot(LF_means_long_recoded,
aes(x = exp_recoded, y = mean, group = condition, fill = condition)) +
geom_bar(position = "dodge", stat = "identity") +
geom_linerange(aes(ymin = ci_lower,
ymax = ci_upper),
position = position_dodge(width = .9)) +
facet_grid(. ~ condition2, scales = "free", drop = TRUE,  space = "free_x") +
scale_fill_discrete(labels = c("one", "three")) +
ylim(0, 1) +
ylab("Prop. basic-level choices") +
xlab("Experiment") +
theme(strip.text = element_text(size = 10),
strip.background = element_rect(fill = "grey"),
legend.position = c(0.92, 0.79),
legend.text = element_text(size = 9),
legend.title = element_text(size = 10),
legend.key.size = unit(.5, "cm"),
legend.background =  element_rect(color = "black", size = 0.5)) +
#ggthemes::theme_few() +
ggthemes::scale_color_solarized()
## key to experiment factors
exp_key <- read_csv("../data/experiment_key.csv") %>%
mutate(order = gsub("\"", "", order))# this is because excel is dumb, grrr
```
exp_key
365 + (365/2)
